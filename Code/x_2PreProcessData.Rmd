---
title: "No or Low Variance Variables"
output: html_notebook
---
snippets

```{r}
ncol <- trainData %>% select(., contains("WAP")) %>% length()

start_col <- trainData %>% select(., contains("WAP")) %>% length()
end_col <- ncol(trainData)
head(trainData[,460:cols])
rm(cols)
```

Characteristics of variables that do not contain much information:
 - Constant or nearly constant (if they have same constant value, no or little variance, there's not much use to models, since the variance is what we can get insight from)
 -- can mess up many methods
 --- cross validation: with little variance, one fold an end up with no variance
 
 
Fortunately, caret contains a utility function called nearZeroVar() for removing such variables to save time during modeling.

nearZeroVar() takes in data x, then looks at the ratio of the most common value to the second most common value, freqCut, and the percentage of distinct values out of the number of total samples, uniqueCut.
The frequency ratio would be near one for well-behaved predictors and very large for highly-unbalanced data.. 



By default, caret uses freqCut = 19 and uniqueCut = 10, which is fairly conservative. I like to be a little more aggressive and use freqCut = 2 and uniqueCut = 20 when calling nearZeroVar().


To identify these types of predictors, the following two metrics can be calculated: * the frequency of the most prevalent value over the second most frequent value (called the “frequency ratio’’), which would be near one for well-behaved predictors and very large for highly-unbalanced data>
# Clean up Data

1. Remove duplicates , Check for NAs

```{r}
# uses dplyr, remove duplicates
trainData <- trainData %>% distinct()

# No missing values in dataset
sum(is.na(trainData))
```

2. Remove Unused attributes

```{r}
# Remove UserID , timestamp, PhoneID from data
trainData <- trainData %>%
  select(-USERID,-PHONEID, -TIMESTAMP, - SPACEID, -RELATIVEPOSITION)
```


```{r}
names(trainData)
```

3.  Recode values

```{r}
trainData[trainData == 100] <- 999
```

Mak
```{r}
#make negatives into positive, e
col_start <- 1
col_end <- ncol(trainData)-4
trainData[,col_start:col_end] <- lapply(trainData[,col_start:col_end], abs)
#trainData[trainData == 999] <- -100

```


```{r}
col_end_final <- ncol(trainData)
head(trainData[509:col_end_final])
```



4. Filter by Building

```{r}
trainData0 <- trainData %>%
  filter(BUILDINGID == 0) %>%
  select(-BUILDINGID)
trainData1 <- trainData %>%
  filter(BUILDINGID == 1)%>%
  select(-BUILDINGID)
trainData2 <- trainData %>%
  filter(BUILDINGID == 2)%>%
  select(-BUILDINGID)
```


4. Building 0

Remove zeroVariance variabels

```{r}
# Look for 
#WAPcols <- trainData0 %>% select(., contains("WAP")) 
nearZeroVarResults <- nearZeroVar(trainData0, names = TRUE, freqCut = 3, uniqueCut = 10, saveMetrics = TRUE)
nearZeroVarResults
# Save zero variance variables into a vector
zeroVAR <- which(nearZeroVarResults$zeroVar == TRUE, useNames = TRUE )
```

```{r}
# Remove WAP variables with zero variance
trainData0 <- trainData0[ , -zeroVAR]
```

```{r}
# Summary
head(trainData0, n = 50)
```

We only want columns that have strong WAP signals. 

Weak signals are of -70dbm or lower. 

In our dataset, weak signals are numbers that are -100. 

We want columns who only have signals from 0 to 70


We will get rid of all WAP columns which only have weak signals. 

To do this, we remove columns whose weakest signal value is -70 or higher (70+ in our dataset.
We took the minimum value of all columns, and filtered out those columns whose minimum value was 70 or higher. 

```{r}
#library(data.table)
mins <- sapply(trainData0, min)
mins_df <- as.data.frame(mins, keep.rownames = TRUE)
mins_df <- setDT(mins_df, keep.rownames = TRUE)
mins_df_filtered <- mins_df %>% 
  filter(rn !="LONGITUDE" & rn != "LATITUDE" & rn != "FLOOR", mins > 70)
```







```{r}
mins_df_name_vec <- c(mins_df_filtered$rn)
```


```{r}
# Keep WAP variables with strong signals only
trainData0 <- trainData0 %>% select(-mins_df_name_vec)
```

```{r}
# filter by floor
trainData0_fl0 <- trainData0 %>%
  filter(FLOOR == 0) %>%
  select(-FLOOR)
```

```{r}

# Reduce variability
#WAPcols <- trainData0 %>% select(., contains("WAP")) 
nearZeroVarResults <- nearZeroVar(trainData0_fl0, names = TRUE, freqCut = 3, uniqueCut = 10, saveMetrics = TRUE)
nearZeroVarResults
# Save zero variance variables into a vector
zeroVAR <- which(nearZeroVarResults$zeroVar == TRUE, useNames = TRUE )
NearzeroVAR <- which(nearZeroVarResults$nzv == TRUE, useNames = TRUE )
NearzeroVAR
# Remove WAP variables with zero variance
trainData0_fl0 <- trainData0_fl0[ , -zeroVAR]
```



#Remove WAP variables with zero variance

Where are our WAPs located? Which WAPs correspond to which buildings?



 
There are too many variables with near zero variance. This is an interesting aspect of this dataset. Because of this, we will just remove WAP variables with zero variance.
      
```{r}
# Remvoe WAP variables with zero variance
trainData <- trainData[ , -zeroVAR]
ncol(trainData)
```

      
```{r}
#clean up environment
rm(nearZeroVAR, nearZeroVarResults,zeroVAR, WAPcols)
```

We went from 529 variables to 470 variables, and from 14954 rows to 14449 rows. 

This is still a lot. 

# Turn Categorical Variables into Factors

```{r}
#unique(trainData$SPACEID)
unique(trainData$RELATIVEPOSITION)

```


```{r}
# turn into factors

trainData$BUILDINGID <- as.factor(trainData$BUILDINGID)
trainData$FLOOR <- as.factor(trainData$FLOOR)
trainData$SPACEID <- as.factor(trainData$SPACEID)
trainData$RELATIVEPOSITION <- as.factor(trainData$RELATIVEPOSITION)


```





# Split data by building 0 


```{r}
trainData0 <- trainData %>%
  filter(BUILDINGID == 0)
trainData1 <- trainData %>%
  filter(BUILDINGID == 1)
trainData2 <- trainData %>%
  filter(BUILDINGID == 2)
```
https://www.netspotapp.com/what-is-rssi-level.html

https://www.mist.com/documentation/rssi-values-good-bad-signal-strength/

-70 dbm threshold
```{r}
noWAP_RSSI <- nrow(trainData0)*100
minWAP_RSSI <- -70
```

```{r}
# remove WAP with only 100 values, no signals (probably WAPs for other buildings)
WAPcols <- trainData0 %>% select(., contains("WAP")) %>% colSums() < noWAP_RSSI
trainData0 <- trainData0[,WAPcols== TRUE ]
```

```{r}
summary(trainData0)
```

max(column name)

less than (max(column name))

```{r}
# remove WAP with min values less than -70dnm, weak signals
WAPcols <- trainData0 %>% select(contains("WAP"))  

WAPcols <- summarize_all(WAPcols, max)


#filter_all(mtcars, any_vars(. > 150))

WAPcols[WAPcols[1,] > 70,] 



#trainData0 <- trainData0[,WAPcols== TRUE ]
```

```{r}
summary(trainData0)
```



```{r}
lapply(trainData0, function(x) table(x))
```


Remove phone, user id,and timestamp as variables.

We are not using these as predictor variables. The purpose of this project is to see if identifying user location based on attributes for a shopping mall is feasible. UserID and phoneID will always change. This is for a public setting, and shopping malls have many various users that are always changing. If this was for a company, with permanent employees, we may be able to delve into behavioral patters to identify as well. However, this is for a public setting, therefore, we will remove UserID. Similarly, we could go into another study to see how the type of phone would affect the stregth of wifi fingerprinting, but that is out of the scope of this project. 

Similarly, we don't want to use time as a predictor. There should be no logical relationship with that. Well, maybe. Sometimes, lunch hours might have more people in a 'food court', but to effectively study time, we would need to observe it on data from a controlled experiment. Based on the documentation, the data wasn't recorded from natural time patterns. Furthermore, the exact purpose of this project is to see if wifi fingerprinting is feasible. Can we tell location based on wifi fingerprints, not time patterns. 

Per the documentation,spaceID and relativePosition, won't be available in real-life. Since we are looking for feasibility, we will not use these two variables either.

```{r}
# Remove UserID , timestamp, PhoneID from data
trainData0 <- trainData %>%
  select(-USERID,-PHONEID, -TIMESTAMP, - SPACEID, -RELATIVEPOSITION)
cols <- ncol(trainData)
head(trainData[,460:cols])
rm(cols)

trainData1 <- trainData %>%
  select(-USERID,-PHONEID, -TIMESTAMP, - SPACEID, -RELATIVEPOSITION)
cols <- ncol(trainData)
head(trainData[,460:cols])
rm(cols)

trainData2 <- trainData %>%
  select(-USERID,-PHONEID, -TIMESTAMP, - SPACEID, -RELATIVEPOSITION)
cols <- ncol(trainData)
head(trainData[,460:cols])
rm(cols)
```
# Visualize WAP of building 0

```{r}
plotBuilding0 <- plot_ly(trainData0, x = ~LATITUDE, y = ~LONGITUDE, z = ~FLOOR, color = ~FLOOR, colors = c('#BF382A', '#0C4B8E'))
plotBuilding0 <- plotBuilding0 %>% add_markers()
plotBuilding0 <- plotBuilding0 %>% layout(title = "Building Zero", scene = list(xaxis = list(title = 'Latitude'),
                     yaxis = list(title = 'Longitude'),
                     zaxis = list(title = 'Floor Level', dtick = 1)))

plotBuilding0
```


```{r}
plotBuilding1 <- plot_ly(trainData1, x = ~LATITUDE, y = ~LONGITUDE, z = ~FLOOR, color = ~FLOOR, colors = c('#BF382A', '#0C4B8E'))
plotBuilding0 <- plotBuilding0 %>% add_markers()
plotBuilding0 <- plotBuilding0 %>% layout(title = "Building One", scene = list(xaxis = list(title = 'Latitude'),
                     yaxis = list(title = 'Longitude'),
                     zaxis = list(title = 'Floor Level', dtick = 7)))

plotBuilding1
```

```{r}
plotBuilding2 <- plot_ly(trainData2, x = ~LATITUDE, y = ~LONGITUDE, z = ~FLOOR, color = ~FLOOR, colors = c('#BF382A', '#0C4B8E'))
plotBuilding0 <- plotBuilding0 %>% add_markers()
plotBuilding0 <- plotBuilding0 %>% layout(title = "Building Two", scene = list(xaxis = list(title = 'Latitude'),
                     yaxis = list(title = 'Longitude'),
                     zaxis = list(title = 'Floor Level', dtick = 1)))

plotBuilding2
```

There are not data points on all locations on all floors. 

# Single Unit Identifiers

Combine floor and building

```{r}
unique(trainData$FLOOR)
unique(trainData$BUILDINGID)

```

```{r}
table(trainData$BUILDINGID, trainData$FLOOR)
```
Building 0 and Building 1 only have 4 floors.
Building 2 has 5 floors.

Floor
Building0: 00 , 01, 02, 03, 04
Building1: 10, 11, 12, 13, 14
Building2: 20, 21, 22, 22, 24

We will also combine relative position and space id, to make on
```{r}
#uses tidyr
# combine building and floor, and then remove building and floor from train data
trainData <- trainData %>%
  mutate( building_Floor = paste(BUILDINGID, FLOOR, sep ="") )%>%
  select(-BUILDINGID, -FLOOR)
```

Remove phone, user id,and timestamp as variables.

We are not using these as predictor variables. The purpose of this project is to see if identifying user location based on attributes for a shopping mall is feasible. UserID and phoneID will always change. This is for a public setting, and shopping malls have many various users that are always changing. If this was for a company, with permanent employees, we may be able to delve into behavioral patters to identify as well. However, this is for a public setting, therefore, we will remove UserID. Similarly, we could go into another study to see how the type of phone would affect the stregth of wifi fingerprinting, but that is out of the scope of this project. 

Similarly, we don't want to use time as a predictor. There should be no logical relationship with that. Well, maybe. Sometimes, lunch hours might have more people in a 'food court', but to effectively study time, we would need to observe it on data from a controlled experiment. Based on the documentation, the data wasn't recorded from natural time patterns. Furthermore, the exact purpose of this project is to see if wifi fingerprinting is feasible. Can we tell location based on wifi fingerprints, not time patterns. 

Per the documentation,spaceID and relativePosition, won't be available in real-life. Since we are looking for feasibility, we will not use these two variables either.

```{r}
# Remove UserID , timestamp, PhoneID from data
trainData <- trainData %>%
  select(-USERID,-PHONEID, -TIMESTAMP, - SPACEID, -RELATIVEPOSITION)
cols <- ncol(trainData)
head(trainData[,460:cols])
rm(cols)
```

# filtering data, traindata0
remove any columns whose WAP values are lower than 95


which columns have values that are only less than 95?

for every column from 1 to 520, check if there are only row values less than 95

If so, remove

```{r}
summary(trainData0)
```


```{r}
sample1 <- trainData0 %>%
  filter()


noWAP_RSSI <- 520*100
# Create Total RSSI column and remove all rows where all values equal to 100
trainData <- trainData %>%
  mutate(totalRSSI = rowSums(select(., contains("WAP")))) %>%
  filter(totalRSSI != noWAP_RSSI) %>%
  select(-totalRSSI)
# remove noWAP_RSSI from environment
rm(noWAP_RSSI)
```




```{r}
# Fit model
# Logistic regression model, which allows for more varied types
# We use parameter "binomial" to specify that we want to do logistic rather than linear regression
#glm_model <- glm(Target ~ ., family = "binomial", trainData)
```

```{r}
# Predict Model
#predict(glm_model, test, type = "response")
```

We will use KNN because it's for classification and has been used for mapping problems (see Boston Housing Dataset). This technique  seems natural to dataset/

